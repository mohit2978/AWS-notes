# Lecture 37
## S3 Static Website Hosting
Static websites are web applications that consist of fixed, unchanging content; unlike dynamic websites, where the content is generated on the server dynamically in response to user requests, static websites present pre-built, unaltered HTML, CSS, and JavaScript files directly to the user's web browser.

### Static website hosting in S3#
S3 buckets allow us to upload static HTML web pages and serve them on region-specific website endpoints. The general format of an S3 endpoint is of two types, depending upon the region.

- s3-website dash (-) Region: http://bucket-name.s3-website-Region.amazonaws.com

- s3-website dot (.) Region: http://bucket-name.s3-website.Region.amazonaws.com

S3 website hosting supports custom domain configurations, allowing us to associate branded URLs with their hosted content. If we have registered domains, we can use our custom domain names by recording DNS CNAME. Another method is to use custom domain names with Route 53 and serve the website content.

### Enabling static website hosting#
Static website hosting feature requires an index documnet and IAM permissions to access bucket content publically. We can enable static website hosting using the Amazon Management Console, REST API, AWS SDKs, AWS CLI, and AWS CloudFormation.

The index document is an HTML file rendered by the S3 website endpoint. S3 requires the name of the index document. If we have other web pages in the directories, the name of the index document must be consistent across them.

For example, if we want to name our index document as my_index.html and host multiple pages in different directories, each folder should have a my_index.html. Suppose we have a bucket in us-east-1 the region named my-bucket with a folder, /events. Then, it should contain an index document that will be returned when we access the website endpoint.

![alt text](image-1.png)

### Bucket permissions
To host a static website using S3 buckets, we must permit public access to the bucket. Disabling Block Public Access and adding a bucket policy to restrict actions is the most commonly used method to manage public access. If we don't want to disable Block Public Access settings for the bucket but still want our website to be public, we can create an Amazon CloudFront distribution to serve our static website. Another way could be to use ACLs, but only if necessary. Especially if the owner of the bucket does not necessarily own the objects in the bucket, then one can use ACL to give full access to the owner.

### Use case: Out-of-service on the website front page
Static website hosting is commonly used for personal portfolios, temporary promotional websites, data offloading, and more at far less cost than typical website hosting.

Let’s consider one use case to understand its usability further. Suppose there is an e-services website that has undergone some server maintenance and will not be able to provide its services for some time. As a good practice we want to display an out-of-service message on the website front page. However, since the main server is down, we could not render the customized page. Renting out another server, say EC2 instance, to display a customized out-of-service page will be considerably expensive.

Fortunately, we can leverage the static website hosting feature of S3 buckets in such scenarios. Simply upload the customized out-of-service HTML page to the bucket and configure it as the index document. Then, redirect the traffic to the website’s endpoint to the S3 bucket website endpoint.

![alt text](image-2.png)

## Lifecycle Management 
Amazon S3 (Simple Storage Service) is an economical object storage solution. However, since it is mostly used for backups, logs, and archives, data can quickly pile up and incur a large cost. We have already discussed that S3 offers a suite of storage options tailored to meet the required latency at a minimal cost. Using the right storage solution for the use case, enterprises and developers can considerably save their storage costs.

![alt text](image.png)

But what if our data access requirements vary? For example, consider a software application that stores logs on an S3 bucket and requires them frequently for a week. After that, we can delete them. Similarly, some documents might require frequent access in the initial few weeks or months and would be rarely accessed as time passes by. In such a situation, it can be a hassle to manually transmit objects between multiple storage classes to optimize cost.

To automate the transitioning between the storage classes or deletion of objects based on the configurations, S3 provides S3 Lifecycles. An S3 Lifecycle Configuration is a set of rules that defines when to transmit an object or delete it. There are two types of S3 Configuration rules:

- Transition: These rules define the transition of objects between different storage classes.
e.g . after 90 days move to Clacier

- Expiration: These rules define the deletion of objects from a storage class. e.g. delete after 90 days!

> These rules can be for both current version and also for non current version!! 

> Features in s3 is created at bucket level but applied at object level!!

> we have lifecycle rules!! we can have rules for entire bucket as well as prefix or subfolder!!

We can configure S3 Lifecycles by determining the access frequency of the objects in the bucket. To facilitate developers determine the access patterns of objects in a bucket, S3 provides two options:

- S3 Storage lens: Storage lens works at the AWS organization level, the highest level in the hierarchy, all the way down to the account level, bucket level, and even prefix (subfolder) level. We can use it to get a snapshot of the number of buckets in an organization, the storage space they are taking up, and the cost. Also, it created a bubble chart based on the retrieval rate of each bucket, helping in decoding the information and choosing the right configurations for the S3 Lifecycle.

- S3 Storage class analysis: Storage class analysis provides access and usage patterns information at the bucket level.


>Who is accessing the buckets you can see logs in server access logs , we can put the logs in separate buckets . Server access logs are on bucket level!!

> Object level logs we put in cloudtrail!!

Logs are generally put with no spaces ,so no one can read it ,so we use another service called as Athena!!
## AWS Athena
Amazon Athena is an interactive query service provided by Amazon Web Services (AWS) that allows us to analyze data stored in Amazon Simple Storage Service (Amazon S3) using standard SQL queries. It enables us to quickly and easily query large-scale datasets without having to set up or manage any infrastructure.

### Service integrations
Amazon Athena can be conveniently integrated with several Amazon services. However, the trademark use case is with S3 buckets.

Since Athena is serverless and compatible with multiple formats, it is ideal for performing ad-hoc SQL queries on data stored in S3. It is commonly used for quick data exploration, troubleshooting (e.g., analyzing web logs), or any scenario where we need to analyze S3 data using interactive SQL queries without managing servers.

It  can also integrate with other AWS resources:

- Amazon QuickSight: Generate data visualizations from Athena query results for easy data exploration.

- Business intelligence tools & SQL clients: Integrate Athena with existing BI tools or SQL clients using JDBC/ODBC drivers for broader data analysis capabilities.

- AWS Glue Data Catalog: Leverage Glue’s metadata store for data in the S3 bucket to define tables and manage data in Athena. This metadata is accessible across the AWS account and integrates with AWS Glue’s ETL (Extract, Transform, Load) and data discovery features.

### Performance optimization with Athena
While executing queries on an S3 bucket, we can face latency or failures due to multiple queries executing simultaneously. We can use S3 throttling at the service level to limit the rate of queries executing simultaneously. Below are some ways we can store our data in S3 to optimize the performance of Athena.

- Columnar data: It decreases the read time of the query. Apache Parquet and Apache ORC are the most commonly used columnar data stores. We can use AWS Glue to convert to these formats.

- Compressed data: Compressing data stored in Amazon S3 reduces the data transferred over the network when querying with Athena.

- Partition datasets: By partitioning data, we limit the amount of data accessed at any given time to improve the speed of the query and avoid S3 throttling.

### Benefits of AWS Athena#
Amazon Athena provides the following benefits to the users:

- Simplified data analysis: Athena streamlines data analysis by removing server management complexities. We can focus on writing queries and extracting insights from the data.

- Faster time to insights: With its serverless architecture and ease of use, Athena allows us to get started with data analysis quickly. No need for a lengthy infrastructure setup or data manipulation before querying.

- Cost optimization: The pay-per-query model ensures we only pay for the queries we run, making Athena a cost-effective solution for exploratory analysis or occasional queries.

- Familiar interface: Using standard SQL makes Athena accessible to a wider range of users, including data analysts and scientists already comfortable with SQL.

## Secure Objects in an S3 Bucket
S3 offers multiple ways to secure objects in the bucket by either restricting objects in the bucket or applying a standardized security pattern on all of the objects. Let’s dive to learn multiple ways we can protect objects stored in our bucket.

### S3 Object Locks
S3 Object Locks prevent overwriting or deletion of an object. It is based on the WORM (write-one-read-many) model. Object Locks only work on versioning-enabled S3 buckets. We can lock a specific object version, which associates the lock information with the metadata of the version.

Object Lock provides two ways to manage the retention: retention period and legal holds.

> we can even lock the object by ObjectLock , after locking no one can modify or delete object!! we can lock permanently or for specific period of time!!

### Retention period
We lock the object for a specified amount of time. We can set up a unique or default object retention period on a bucket. Furthermore, we can set up maximum and minimum allowed retention periods using the s3:object-lock-remaining-retention-days condition key in the bucket policy. This ensures that users can only specify retention periods that fall within a predefined range, enforcing data retention policies and preventing users from setting overly restrictive or insufficient retention periods.

This mode offers two ways to lock an object:

- Compliance: When an object is locked in compliance mode, it can not be overwritten or deleted even the root user. Also, we can not shorten the retention period or modify the retention mode.

- Governance: When an object is locked in governance mode, it can only be overwritten or modified by users with special permissions.

>Tip: Testing out Object Locks in governance mode is a good practice before locking objects in compliance mode.

### Legal hold

We lock the object for an indefinite amount of time. It remains in action until we explicitly remove it. Legal holds are independent of retention periods.

 For example, consider an object with a retention period and legal hold. If the retention period expires before removing the legal hold, the object will remain protected. Similarly, if the legal hold is removed before the retention period expires, the object can not be overwritten or deleted.

> Important: A locked version of an object can not be deleted by S3 life cycle policies. It maintains the object lock irrespective of the storage class.

## S3 Glacier Vault Lock
A vault is defined as a container for archiving objects. S3 Glacier supports vault operations specific to an AWS region. A vault can only be deleted if there are no objects in it.

S3 Vaults allow us to apply S3 Vault Locks to control vault access. Vault Locks are defined in the IAM policy document to enforce compliance controls.

 However, they are different from the vault access policy. Vault Lock policies can be locked to prevent any changes in the future. They are commonly used for compliance purposes and are not subjected to frequent changes. On the other hand, vault access policies are subjected to frequent modifications to grant temporary access to certain users.
## Encryption in general


1. In transit encryption : (when tranasition we do encryption) can be done by HTTPS /ACM
2. data at rest encryption : ( when data is at rest we do encryption like data stored in s3)


ACM is where we create https certificate and use for in-transit encryption

KMS is where you can create encryption keys and use for data at rest

 ## S3 objects encryption

Data stored in S3 buckets needs to be protected at rest while stored in disks and in transit. To protect data when it is being transmitted to and from S3 buckets, we can use Secure Socket Layer/Transport Layer Security (SSL/TLS) or client-side encryption. In the context of Amazon S3, it’s important to note that SSL/TLS is primarily utilized for securing data in transit rather than for securing data stored within Amazon S3 buckets.

S3 offers the following two main options to protect data:


### Server-side encryption(SSE)

By default, S3 encrypts data before storing them on disks using S3 managed keys (SSE-S3). These objects are automatically encrypted at no additional cost and performance latency.

We can also specify a different type of encryption using:

- Server-si- de encryption with AWS Key Management Service (AWS KMS) keys (SSE-KMS)

- Dual-layer server-side encryption with AWS KMS keys (DSSE-KMS)

- Server-side encryption with customer-provided keys (SSE-C)

- SSE-S3 (uses AWS managed key)

all uses (Advanced encryption standard) AES-256 

> By default buckets are encrypted by SSE-S3!!

We can specify the type of encryption with the PUT request.

### Client-side encryption
We can use client-side encryption to ensure that the data is secure at rest on the client side and when transmitted from the client to the S3 bucket. In this type of encryption, the S3 bucket receives the encrypted data and can not decrypt it. Also, it can not detect if the data is encrypted and treats it as regular objects.

S3 offers a client-side encryption library called Amazon S3 Encryption Client. 
It’s designed to seamlessly integrate with AWS services, enabling developers to easily encrypt data on the client side before uploading it to the S3 bucket.

## CORS (Cross origin resouce sharing)

Multiple origins (buckets) resouces(objects) sharing,

we have a bucket we have index.html and puppy.jpg index.html use puppy.jpg by url of puppy.jpg!! 

but suppose both are in different bucket then index.html will not get puppy.jpg as different origin!!Cross origin resources are not shared !! so for this you need to enable cors in bucket where puppy.jpg is there!! and in that you give permission to bucket where index.html is present!!

if you get error CORS is blocked you need to enable it!!

## Replication
Amazon S3 buckets allow us to asynchronously replicate objects from the source to the target bucket. S3 allows us to replicate objects between buckets in similar or different accounts and regions. Also, we can set up multiple destination buckets for replication.

A commonly asked question is why we use S3 replication when we can conveniently upload objects manually to replicate them. Well, S3 replication is helpful in a bunch of cases:

- Replicate objects with metadata: S3 replication copies objects along with the original metadata, such as creation made, modification date, and more.

- Time-bound replication: To meet the compliance requirements, we might need to replicate objects in a limited time. S3 replication offers Replication Time Control (RTC) which ensures replication of 99% of the object within 15 minutes.

- Replicate to multiple storage classes: We can replicate data quickly to a different storage class to optimize costs.

- Data redundancy: Replicate data across regions to improve network latency and failure tolerance.

### Cross-Region Replication
Cross-Region Replication (CRR) is a live replication technique where we can 
replicate objects across regions as they are written into the bucket. It is commonly used to achieve data redundancy.

For example, consider a multinational company that relies heavily on cloud storage for its critical data and applications. The company wants to ensure that its data is resilient to regional outages, natural disasters, or any unforeseen events that could impact the availability of its primary data center.

To address this, the company employs Cross-Region Replication in Amazon S3. They set up replication rules to automatically copy objects from their primary S3 bucket in one AWS region to a secondary S3 bucket in a geographically distant region. This secondary region can act as a backup and minimize latency for the compute services in the region.

![alt text](image-3.png)

Another common use case of CRR is to achieve data redundancy or decrease data access latency by bringing the data close to the users accessing the bucket from various regions.

In CRR rule we create at source , we mention the destination where file will be replicated!!

Only the content created at or after the creation of rule will be copied to other bucket , contents before
the rule creation will not be replicated!! 

After creating CRR rule s3 will ask do you want to replicate existing object!!


### Same-Region Replication
Same-Region Replication (SRR), as the name suggests, allows us 
to replicate objects in the same region. This type of replication is asynchronous, 
meaning that changes made to the source bucket are not immediately reflected in the destination bucket. SRR achieves eventual consistency but is not ideal for synchronous real-time replication scenarios.

Same-region replication is typically used to ensure high availability and protect against the potential loss of data. Also, we can use it to aggregate objects, such as logs, from multiple source buckets across a region into one destination bucket.

![alt text](image-4.png)

>in order to CRR or SRR to work versioning must be enabled

>  CRR  can be in same account or in different account !!

> CRR or SRR can be enabled for both bucket as well as prefix also!!

> S3 cost depends on size of bucket and data transfer!!

### S3 Batch Replication

S3 Batch Replication allows us to replicate the existing objects in a bucket. Unlike SRR and CRR, 
which replicate objects as they are added to the bucket, S3 Batch Replication is 
an on-demand process. We can use Batch Replications for the following scenarios:

- Replicate the existing objects before configuring SRR or CRR.

- Replicate objects that failed to replicate using the FAILED replication status.

- Replicate the replicas of the objects as they can only be replicated using S3 Batch Operations.

- Replicate objects that have already replicated to a different region.

used for replicate in batches!! suppose we have 10k objects , it will create batch job !!

### Two-way or bidirectional replication
Bidirectional replication, also known as two-way replication, synchronizes changes made at both the source and destination bi-directionally. In this setup, data modifications can occur independently at each end, and the changes are propagated in both directions to maintain consistency between the systems.

To understand its use case, consider a scenario where a sales team operates in different regions, each with its database storing sales orders. Bidirectional replication allows sales representatives in each region to independently enter and update sales orders. The changes made in one region are then replicated bidirectionally to other regions, ensuring the sales order data is consistent across all locations.

![alt text](image-5.png)

![alt text](image-6.png)

### Replication configuration
A replication configuration is the set of rules and settings that define how object replication is managed between source and destination S3 buckets. It is written in XML format and contains the following important elements:

- Status (mandatory): Specifies if the rule is Enabled or Disabled. S3 does not consider a disabled rule while replicating.

- IAMRole: The S3 buckets require permissions to get the bucket configurations and read-write the objects. Thus, the S3 should have the necessary permissions to replicate objects from source to destination buckets. We can restrict an IAM role to allow replication of all the objects or a subset of objects based on filters.

- Priority (mandatory): Determines which rule will be given priority when there is a conflict among multiple rules with the same destination bucket.

- Destination (mandatory): Specifies the destination bucket(s).

- Filters: Filters allow us to replicate only a subset of objects. We can configure filters based on the key prefixes and tags of an object. For example, we have two folders jun an S3 bucket named /images and /documents, and we only want to replicate the /images folders. To handle this case, we can configure a filter with the prefix images. Thus, all the objects with the prefix images in their URL will be replicated to the destination bucket.

- DeleteMarkerReplication (mandatory): S3 stores delete markers when we delete an object in a version-enabled S3 bucket. However, in a configuration profile, we can specify if we want to replicate delete markers or not.

>Note: To configure replication on an S3 bucket, we must enable versioning on both source and destination buckets.

S3 buckets offer multiple options to replicate the objects. Let’s learn about each of these options.

## S3 Data consistency Models

>Note : upload to s3 is PUT request

first we write(put) and then read (get) . we read after write!!

S3 data consistency model states that it provides __read-after-write 
consistency for PUTS of new objects__  with one caveat. 
The caveat is that if you make a HEAD or GET request to an object before its created, 
then create the object shortly after that, a
 subsequent GET might not return the object due to eventual consistency (might get 200 OK or 404 not found)
 . And it offers eventual consistency for overwrite PUTS and DELETES in all Regions.
 2 consistency models:

### Read-after-write consistency

S3 provides read after write consistency for new objects. 
This indicates that read operation on an object after write operation will always be a success. 
So, any new objects that are created will be replicated across multiple availability zones 
before returning success.

```text
PUT /myObjects/file-1.jpg — 200 
GET /myObjects/file-1.jpg — 200
```
There is a caveat here - if you make a HEAD or GET request to an object before its created, 
then create the object shortly after that, a subsequent GET might not return the object.

```text
GET /myObjects/file-1.jpg — 404
PUT /myObjects/file-1.jpg — 200
GET /myObjects/file-1.jpg — 404
```
 see even we PUT the file is still returning 404!!
### Eventual consistency

S3 offers eventual consistency for overwrite PUTS and DELETES. 
This indicates any GET/HEAD operation shortly after overwrite PUT and DELETE 
may or may not see the updated value.

```text
PUT /myObjects/file-2.jpg — 200
DELETE /myObjects/file-2.jpg — 200
GET /myObjects/file-2.jpg — (200 or 404)
PUT /myObjects/file-1.jpg — 200
PUT /myObjects/file-1.jpg — 200
GET /myObjects/file-1.jpg — 200 (can be old content or new content)
```
Update and Delete operations return success before propagating the change to all the object’s availability zones which results in inconsistent behavior for operations executed shortly after update/delete. Once the changes are propagated to all availability zones, it will be consistent behavior.

PUT and DELETE data is propagated slowly eventually!! 

__overwrite consistency of PUTS and DELETES eventually__

One thing to note here is, the above consistency model is independent of client sending requests. GET operations can be from the same or different from the client making PUT, DELETE requests. 

## Presigned URLs

Presigned URLs are used to grant temporary access to the bucket. It is a time-bound URL that can be used to share and upload objects in a bucket.

### How to create a presigned URL
The presigned URL can be created by anyone with valid security credentials. However, the entity with those credentials should have permission to perform the functions the presigned URL is being used for. This entity can be an IAM profile, a user, or a security token service. To create a presigned URL we require:

- Name of the S3 bucket

- The object key (If we are downloading the object using the presigned URL, then this object should be in the bucket. Otherwise, this should be the name of the object being uploaded.

- The HTTP method (GET for downloading and PUT for uploading)

- The time interval after which the presigned URL expires

### Expiration time

Presigned URLs grant temporary credentials and expire after the specified time. The AWS Management Console allows an expiration time of between 1 minute and 12 hours. However, while using the AWS CLI or AWS SDK, we can specify an expiration time of up to seven days.

If we are creating a presigned URL using a temporary security token, and it expires before the scheduled expiration or presigned URL, the URL will expire with the token. Also, if the URL expires while uploading or downloading an object, it will complete the upload and download.

### Share objects using the presigned URL
Presigned URLs allow us to create URLs for GET method which can be used in a code through SDKs or pasted into a browser to download the object. This way, a bucket owner grants temporary access to other users who can have similar privileges as the bucket owner for a limited time. This means that other users can only get the object if the URL creator has permission to get the object. We can create a presigned URL to share objects using Amazon S3 console, AWS Explorer for Visual Studio, or AWS Toolkit for Visual Studio Code.

### Upload objects using the presigned URL
Presigned URLs allow us to create URLs for PUT method, which can be used by a third party to upload objects to the bucket. The other users can only put objects in the bucket if the creator of the URL has the privileges to upload objects to the bucket. We can create the presigned URL to upload an object using AWS SDK and AWS Explorer for Visual Studio.

### Securing presigned URLs #
We have already discussed how presigned URLs can grant access to read and write bucket contents for up to seven days. They are only limited by the permissions of the user. This privilege can be easily exploited; thus, it is extremely important to secure a presigned URL.

- To secure the presigned URLs, we can define bucket and access point policies to only accept the request with the Signature Version 4 (SigV4) less than 10 minutes old. The AWS Signature Version 4 (SigV4) is an AWS authentication mechanism for signing AWS API requests. SigV4 requires clients to include a cryptographic signature in their requests, which AWS services can verify to authenticate the request's origin and integrity.

- Another way to restrict access using presigned URLs is network control. We can add IAM policies to only accept a request if it originates from a specific network ID.


## Tarnsfer accelerator

to tarnsfer some file from one region to other fastly we use this!!

- billable
- uses CDN to transfer

## Requester pay

if you enable it , the Requester who needs to access data will pay , if not enabled bucket owner pays
bucket owner pays for transfer cost and storage cost!!

> Requester cannot be anonymous , he must be authenticated from AWS!!

## S3 event notification

to get notification for anything happening in s3!! here we have 3 targets

1. SNS
2. LAMBDA
3. SQS

## Access Points and Object Lambda
We have already discussed that by default, S3 only allows the bucket owner to read and write to a bucket. We have to explicitly grant permissions to other accounts and users to access a bucket. The S3 access points are one such way to grant access.

### Access Points
S3 Access Points are HTTP endpoints attached to the bucket, which allow us to create multiple access configurations to a single bucket. These endpoints allow us to perform basic operations such as GetObject and PutObject on the entire objects or a subset of objects in the bucket. To manage access, each access point has its own access policy attached. Furthermore, we can control network access and allow the entire internet or a specific VPC to access the bucket.


 supoose we have different prefix in a bucket , and so we need devops people to access devops prefix only,
 sales people to only access sales prefix , so for that we can create access point in s3!!

#### Access policy for access points
The access policy of an endpoint works in conjunction with the access policy of the bucket. For example, if the access point’s policy allows a user to write to a bucket, the bucket policy should allow it as well. Otherwise the user might not be able to write objects to the bucket. However, managing access at multiple places can be difficult, especially with multiple users and access points. To simplify it, we can configure a bucket policy to delegate access to Access Points. This would grant access to the user based on the access policy of the Access Point only. It is ideal for use cases where we do not allow direct access to the bucket.

![alt text](image-7.png)

In addition, each Access Point has its own Block Public Access setting. Thus, when a request arrives at the endpoint, S3 allows access only after checking the Block Public Access setting of the access point, underlying bucket, and bucket owner.

>Access points can be public as well as private

> by defualt object is private but bucket is public ,if required object can be made public!!



